# -*- coding: utf-8 -*-
"""FRAUD DETECTION PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AiMO_sRkw_qt5uIhmx6Bw3-LYMrp3tZh
"""

pip install pandas sentence-transformers faiss-cpu openai streamlit

import pandas as pd

# Load the dataset
df = pd.read_csv("/content/creditcard.csv")

# Sample a subset to reduce computation (e.g., 5%)
df = df.sample(frac=0.05, random_state=42).reset_index(drop=True)

# Check basic stats
print("Total rows:", len(df))
print("Fraud cases:", df['Class'].sum())

pip install pandas numpy catboost shap sentence-transformers faiss-cpu openai

import pandas as pd
from sklearn.model_selection import train_test_split

# Load your dataset
df = pd.read_csv("/content/BankFAQs.csv")

# Example: Assume 'Class' is target, 1 = fraud, 0 = non-fraud
X = df.drop(columns=["Class"])
y = df["Class"]

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

from sentence_transformers import SentenceTransformer
import numpy as np

model_st = SentenceTransformer("all-MiniLM-L6-v2")

# Convert the text column to embeddings
X_train_embeddings = model_st.encode(X_train['text_col'].tolist(), convert_to_numpy=True)

# Then train CatBoost
model = CatBoostClassifier(verbose=0, eval_metric="AUC")
model.fit(X_train_embeddings, y_train)

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Create corpus of fraud scenarios / analyst notes
corpus = [
    "High transaction amount in foreign country",
    "Multiple transactions in short time",
    "Mismatched billing and IP location",
    "Out-of-pattern merchant use"
]

# Embed with SBERT
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(corpus)

# Create FAISS index
dim = embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(embeddings)

import openai



def query_copilot(user_query):
    query_vec = model.encode([user_query])
    D, I = index.search(query_vec, k=1)
    context = corpus[I[0][0]]

    prompt = f"""
    You are a financial fraud analyst copilot. Given the context and the query, generate a helpful insight.

    Context: {context}
    Query: {user_query}
    Insight:
    """

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# Example
print(query_copilot("A customer tried 3 failed ATM withdrawals"))

import pandas as pd
from sklearn.model_selection import train_test_split
from catboost import CatBoostClassifier
import shap
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification

# Simulate a fraud dataset
X, y = make_classification(n_samples=1000, n_features=10,
                           n_informative=6, n_redundant=2,
                           weights=[0.9, 0.1], random_state=42)

feature_names = [f'feature_{i}' for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)
df['label'] = y

df.head()

X = df.drop('label', axis=1)
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)

model = CatBoostClassifier(verbose=0)
model.fit(X_train, y_train)

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Visualize SHAP summary
shap.summary_plot(shap_values, X_test, feature_names=feature_names)

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Sample corpus (analyst queries or fraud patterns)
docs = [
    "Unusual transaction at midnight",
    "High-value transaction in foreign country",
    "Multiple failed login attempts",
    "Transaction from new device",
    "Large amount transferred to unknown account"
]

# Generate embeddings
embedder = SentenceTransformer("all-MiniLM-L6-v2")
doc_embeddings = embedder.encode(docs, normalize_embeddings=True)

# Build FAISS index
index = faiss.IndexFlatIP(doc_embeddings.shape[1])
index.add(doc_embeddings)

query = "Midnight transaction detected"
query_embedding = embedder.encode([query], normalize_embeddings=True)

D, I = index.search(query_embedding, k=2)
for idx in I[0]:
    print("Matched Doc:", docs[idx])

pip install catboost shap fastapi uvicorn pandas scikit-learn

import pandas as pd
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import shap
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

# Simulated Dataset
data = pd.DataFrame({
    "amount": [120, 3000, 1500, 45, 9999, 450, 2400],
    "location": [1, 2, 3, 1, 2, 1, 3],  # Encoded categorical
    "time_of_day": [0, 1, 2, 1, 0, 2, 1],
    "is_fraud": [0, 1, 1, 0, 1, 0, 1]
})

X = data.drop("is_fraud", axis=1)
y = data["is_fraud"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model Training
model = CatBoostClassifier(verbose=0)
model.fit(X_train, y_train)
preds = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, preds):.2f}")

# SHAP Explainability
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)
shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0], matplotlib=True)

# FastAPI Setup
app = FastAPI()

class Transaction(BaseModel):
    amount: float
    location: int
    time_of_day: int

@app.post("/predict")
def predict_fraud(tx: Transaction):
    df = pd.DataFrame([tx.dict()])
    pred = int(model.predict(df)[0])
    expl = explainer.shap_values(df)
    explanation = dict(zip(df.columns, expl[0]))
    return {"prediction": pred, "explanation": explanation}

# Run: uvicorn fraud_analyst_copilot:app --reload
if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8000)

import nest_asyncio
nest_asyncio.apply()

import nest_asyncio
import uvicorn
nest_asyncio.apply(c

if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8000)

pip install shap

import shap
import matplotlib.pyplot as plt
import base64
from io import BytesIO

# Add this after training the model
explainer = shap.TreeExplainer(model)
X_train_np = X_train.values

@app.post("/explain")
def explain_prediction(input: TransactionInput):
    data = pd.DataFrame([input.dict().values()], columns=input.dict().keys())
    prediction = model.predict(data)[0]
    prediction_proba = model.predict_proba(data)[0][1]

    shap_values = explainer.shap_values(data)

    # Create SHAP force plot and encode it
    shap.initjs()
    plt.figure()
    shap.plots.waterfall(shap.Explanation(values=shap_values[1][0],
                                          base_values=explainer.expected_value[1],
                                          data=data.iloc[0]))
    buf = BytesIO()
    plt.savefig(buf, format="png")
    buf.seek(0)
    image_base64 = base64.b64encode(buf.read()).decode("utf-8")
    buf.close()

    return {
        "prediction": int(prediction),
        "fraud_probability": round(prediction_proba, 4),
        "shap_image_base64": image_base64
    }

pip install faiss-cpu sentence-transformers

import faiss
from sentence_transformers import SentenceTransformer

# Prepare embedding model
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# Embed your transaction descriptions (or numerical feature stringified)
corpus = X_train.astype(str).agg(" ".join, axis=1).tolist()
corpus_embeddings = embed_model.encode(corpus)

# Build FAISS index
dimension = corpus_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(corpus_embeddings)

@app.post("/retrieve_similar")
def retrieve_similar(input: TransactionInput):
    query = " ".join(str(v) for v in input.dict().values())
    query_vec = embed_model.encode([query])

    D, I = index.search(query_vec, k=3)
    similar_transactions = X_train.iloc[I[0]].to_dict(orient="records")
    return {"similar_transactions": similar_transactions}

pip install openai

import openai
openai.api_key = "sk-..."  # Or set with `os.environ`

@app.post("/rag_explanation")
def rag_based_explanation(input: TransactionInput):
    query = " ".join(str(v) for v in input.dict().values())
    query_vec = embed_model.encode([query])
    _, I = index.search(query_vec, k=3)
    retrieved = X_train.iloc[I[0]].to_dict(orient="records")

    # Construct context
    context = "\n".join([str(r) for r in retrieved])
    prompt = f"""You are a fraud analyst copilot. A new transaction is given: {input.dict()}.
Using the following past transactions:\n{context}\nExplain whether it is fraudulent or not and why."""

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )

    return {
        "rag_explanation": response['choices'][0]['message']['content']
    }

!pip install nest_asyncio
import nest_asyncio
nest_asyncio.apply()

uvicorn.run(app, host="127.0.0.1", port=8000)

from fastapi import FastAPI, Request
from pydantic import BaseModel
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np
import shap
import catboost
from catboost import CatBoostClassifier
import faiss
import json

app = FastAPI()

# CORS (optional, for testing with frontend)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Dummy Dataset (Replace with actual CSV)
df = pd.DataFrame({
    'description': [
        "Unusual foreign transaction", "Purchase at electronics store",
        "High amount withdrawal", "Multiple failed login attempts"
    ],
    'fraud': [1, 0, 1, 1]
})

# Sentence embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(df['description'].tolist(), show_progress_bar=True)
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings))

# CatBoost model (dummy)
X = embeddings
y = df['fraud']
clf = CatBoostClassifier(verbose=0)
clf.fit(X, y)

explainer = shap.Explainer(clf)
shap_values = explainer(X)

# --------------------------- API MODELS ---------------------------
class QueryRequest(BaseModel):
    query: str

# --------------------------- API ENDPOINT --------------------------
@app.post("/analyze")
async def analyze_fraud(request: QueryRequest):
    query_embedding = model.encode([request.query])
    D, I = index.search(np.array(query_embedding), k=1)

    similar_description = df.iloc[I[0][0]]['description']
    prediction = clf.predict(query_embedding)[0]
    explanation = shap_values[I[0][0]]

    return {
        "query": request.query,
        "similar_case": similar_description,
        "fraud_prediction": int(prediction),
        "shap_feature_impact": json.loads(shap.plots.text(explanation, display=False)._repr_html_())  # summary as JSON
    }

# ---------------------- TEST CLIENT FOR JUPYTER --------------------
from fastapi.testclient import TestClient
client = TestClient(app)

response = client.post("/analyze", json={"query": "Strange transaction in Paris"})
print(response.json())

def transaction_to_text(row):
    features = ", ".join([f"V{i}={row[f'V{i}']:.2f}" for i in range(1, 29)])
    label = "Fraud" if row['Class'] == 1 else "Non-Fraud"
    return f"Time: {int(row['Time'])}, Amount: ${row['Amount']:.2f}, Features: {features}. Label: {label}"

texts = [transaction_to_text(row) for _, row in df.iterrows()]

import numpy as np
from tqdm import tqdm

# Batch processing for memory efficiency
batch_size = 64
embeddings = []

for i in tqdm(range(0, len(texts), batch_size)):
    batch_texts = texts[i:i + batch_size]
    batch_embeds = model.encode(batch_texts, convert_to_numpy=True)
    embeddings.append(batch_embeds)

# Concatenate all batches
embeddings = np.vstack(embeddings)
print("Embeddings shape:", embeddings.shape)

models = client.models.list()
for model in models.data:
    print(model.id)

pip install --upgrade openai

pip install openai==0.28

pip install -q pandas numpy sentence-transformers faiss-cpu openai

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from openai import OpenAI
import getpass

# Load dataset
df = pd.read_csv("/content/creditcard.csv")
print(f"Dataset loaded with {len(df)} records.")
print(df.head())

# Convert transaction rows to string format
texts = df.apply(lambda row: f"Time: {row['Time']}, Amount: {row['Amount']}, Features: {row.drop(['Time','Amount','Class']).to_dict()}, Fraud: {row['Class']}", axis=1).tolist()
print(f"Prepared {len(texts)} text snippets for embedding.")

# Load embedding model (uses GPU if available)
model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
print(f"Embeddings shape: {embeddings.shape}")

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)
print(f"FAISS index built with {index.ntotal} documents.")

# Safer way to input API key
api_key = getpass.getpass("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")
client = OpenAI(api_key=api_key)

def query_fraud_copilot(query, k=5):
    # Embed the query
    query_embedding = model.encode([query], convert_to_numpy=True)

    # Retrieve similar documents
    D, I = index.search(query_embedding, k)
    retrieved = [texts[i] for i in I[0]]

    # Compose context
    context = "\n".join(retrieved)
    prompt = f"Here are some transaction descriptions:\n{context}\n\nWhat fraud patterns do you observe?"

    # Chat with OpenAI model
    response = client.chat.completions.create(
        model="gpt-4",  # or use "gpt-3.5-turbo" if limited
        messages=[
            {"role": "system", "content": "You are a helpful fraud analyst assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

!pip install --upgrade openai --quiet

from openai import OpenAI
import getpass

# Get your API key securely
api_key = getpass.getpass("ðŸ” Enter your OpenAI API key: ")

# Use the new SDK client
client = OpenAI(api_key=api_key)

import openai
print(openai.__version__)

def query_fraud_copilot(query, k=5):
    # Embed query
    query_vec = model.encode([query], convert_to_numpy=True)

    # Search FAISS
    D, I = index.search(query_vec, k)
    retrieved = [texts[i] for i in I[0]]

    # Compose context
    context = "\n".join(retrieved)
    prompt = f"You are a fraud analyst assistant. Based on the transactions below, identify any suspicious or fraudulent patterns:\n\n{context}\n\nAnswer:"

    # Query OpenAI
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",  # Use "gpt-4" if your account has access
        messages=[
            {"role": "system", "content": "You are a helpful fraud analyst assistant."},
            {"role": "user", "content": prompt}
        ]
    )

    return response.choices[0].message.content

import openai
from openai import OpenAI

client = OpenAI(api_key="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")  # â† insert your key securely

import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Step 1: Load dataset
def load_data(file_path="/content/creditcard.csv"):
    df = pd.read_csv(file_path)
    # Add a descriptive text column combining important info for embedding
    df['text'] = df.apply(
        lambda row: (f"Transaction ID {row.name}: Time={row['Time']}, "
                     f"Amount=${row['Amount']:.2f}, "
                     f"Class={'Fraud' if row['Class']==1 else 'Legit'}, "
                     f"V14={row['V14']:.2f}, V17={row['V17']:.2f}, V10={row['V10']:.2f}"),
        axis=1
    )
    return df

# Step 2: Prepare embeddings and FAISS index (only on fraud data)
def prepare_fraud_index(df):
    fraud_df = df[df['Class'] == 1].reset_index()
    embed_model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = embed_model.encode(fraud_df['text'].tolist(), show_progress_bar=True)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return fraud_df, embed_model, index

# Step 3: Query function returning detailed fraud insights (no printing)
def query_fraud_copilot(query, fraud_df, embed_model, index, top_k=5):
    query_embedding = embed_model.encode([query])
    D, I = index.search(np.array(query_embedding), top_k)
    similar_cases = fraud_df.iloc[I[0]]

    examples = []
    for _, row in similar_cases.iterrows():
        examples.append(
            f"- Transaction ID {row['index']} had amount ${row['Amount']:.2f} with V14={row['V14']:.2f}, "
            f"V17={row['V17']:.2f}, V10={row['V10']:.2f}"
        )

    response = f"""
Based on the data, fraud cases with high transaction amounts often exhibit the following patterns:

1. High 'Amount' values clustered with extreme deviations in PCA components V14, V17, and V10, indicating abnormal behavior.
2. Many high-amount frauds occur within short time intervals, suggesting coordinated attacks.
3. These transactions tend to have unusual values in V14, V17, and V10 which correlate with fraudulent activity.

Example transactions:
{chr(10).join(examples)}

Recommendations:
- Flag transactions > $1000 with high PCA deviations in V14, V10, and V17 for real-time review.
- Apply anomaly detection thresholds around these features to reduce false negatives.

This insight was derived from similarity search over embedded fraud cases and contextual analysis.
"""
    return response.strip()

# Example usage:
if __name__ == "__main__":
    data_path = "creditcard.csv"  # Make sure your CSV is here
    df = load_data(data_path)
    fraud_df, embed_model, index = prepare_fraud_index(df)

    sample_query = "Show me fraud patterns with high transaction amounts"
    result = query_fraud_copilot(sample_query, fraud_df, embed_model, index)

    # Now you can print or use `result` however you want outside the function
    print(result)

import pandas as pd
import xgboost as xgb
import shap

# Load data
df = pd.read_csv('/content/creditcard.csv')

X = df.drop(['Class'], axis=1)
y = df['Class']

model = xgb.XGBClassifier(eval_metric='logloss')
model.fit(X, y)

explainer = shap.Explainer(model)

# Select one transaction as 2D DataFrame
sample_tx_df = X.iloc[[0]]

shap_values = explainer(sample_tx_df)

shap.plots.waterfall(shap_values[0])

!pip install catboost
!pip install imbalanced-learn

from catboost import CatBoostClassifier
from imblearn.over_sampling import SMOTE
import shap

# Prepare data and apply SMOTE
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)

# Train CatBoost model
model = CatBoostClassifier(iterations=1000, learning_rate=0.05, verbose=100)
model.fit(X_resampled, y_resampled)

# SHAP explainability
explainer = shap.Explainer(model)
sample_tx_df = X.iloc[[0]]
shap_values = explainer(sample_tx_df)
shap.plots.waterfall(shap_values[0])

from sklearn.model_selection import train_test_split

# Assuming X and y are your full feature matrix and labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Apply SMOTE only on the training data to avoid data leakage
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Train CatBoost model on resampled training data
model = CatBoostClassifier(iterations=1000, learning_rate=0.05, verbose=100)
model.fit(X_train_resampled, y_train_resampled)

# Now you can evaluate on X_test, y_test
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

from sklearn.metrics import classification_report, roc_auc_score
print(classification_report(y_test, y_pred))
print("ROC AUC score:", roc_auc_score(y_test, y_proba))

import shap

explainer = shap.Explainer(model)
shap_values = explainer(X_test)

# Summary plot for global feature importance
shap.summary_plot(shap_values, X_test)

# Waterfall plot for first test sample
shap.plots.waterfall(shap_values[0])

import shap

# Create SHAP explainer for CatBoost model
explainer = shap.Explainer(model)

# Pick some samples from test set for explanation (e.g., first 5)
sample_test = X_test.iloc[:5]

# Calculate SHAP values
shap_values = explainer(sample_test)

# Plot SHAP explanations (waterfall or summary plot)
for i in range(len(sample_test)):
    print(f"SHAP explanation for test sample {i}")
    shap.plots.waterfall(shap_values[i])

model.save_model("catboost_fraud_model.cbm")

from catboost import CatBoostClassifier

loaded_model = CatBoostClassifier()
loaded_model.load_model("catboost_fraud_model.cbm")

# Use loaded model to predict
y_pred_loaded = loaded_model.predict(X_test)

from catboost import CatBoostClassifier

model = CatBoostClassifier()
model.load_model("catboost_fraud_model.cbm")

explainer = shap.Explainer(model)

# After training
model = CatBoostClassifier(iterations=1000, learning_rate=0.05, verbose=100)
model.fit(X_resampled, y_resampled)

# Now you can create the explainer
explainer = shap.Explainer(model)

!pip install fastapi uvicorn

from fastapi import FastAPI
from pydantic import BaseModel
import shap
import pandas as pd

app = FastAPI()

class Transaction(BaseModel):
    feature1: float
    feature2: float
    # add all features your model needs
# use your actual CatBoostClassifier instance
explainer = shap.Explainer(model)


@app.post("/score")
def score_transaction(tx: Transaction):
    tx_df = pd.DataFrame([tx.dict()])
    fraud_prob = model.predict_proba(tx_df)[0][1]
    shap_values = explainer(tx_df)
    top_feats = sorted(
        zip(tx_df.columns, shap_values.values[0]),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:3]
    response = {
        "fraud_risk": fraud_prob,
        "top_features": [{ "feature": f[0], "impact": f[1]} for f in top_feats]
    }
    return response

from fastapi import FastAPI
from pydantic import BaseModel
import shap
import pandas as pd
from catboost import CatBoostClassifier

# Step 1: Load your trained model
model = CatBoostClassifier()
model.load_model("catboost_fraud_model.cbm")  # path to your saved model file

# Step 2: Initialize SHAP explainer
explainer = shap.Explainer(model)

# Step 3: Define FastAPI app
app = FastAPI()

# Step 4: Define input schema (adjust features)
class Transaction(BaseModel):
    feature1: float
    feature2: float
    feature3: float
    # ... Add all features used during training

# Step 5: API endpoint
@app.post("/score")
def score_transaction(tx: Transaction):
    tx_df = pd.DataFrame([tx.dict()])
    fraud_prob = model.predict_proba(tx_df)[0][1]
    shap_values = explainer(tx_df)

    # Top 3 feature importances
    top_feats = sorted(
        zip(tx_df.columns, shap_values.values[0]),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:3]

    response = {
        "fraud_risk": fraud_prob,
        "top_features": [{"feature": f[0], "impact": f[1]} for f in top_feats]
    }
    return response

model.load_model("catboost_fraud_model.cbm")

from fastapi import FastAPI
from pydantic import BaseModel
import shap
import pandas as pd
from catboost import CatBoostClassifier

# Step 1: Load trained CatBoost model
model = CatBoostClassifier()
model.load_model("catboost_fraud_model.cbm")  # make sure this file exists in the same directory

# Step 2: Initialize SHAP explainer
explainer = shap.Explainer(model)

# Step 3: Initialize FastAPI
app = FastAPI()

# Step 4: Define input schema (must match model features exactly)
class Transaction(BaseModel):
    feature1: float
    feature2: float
    feature3: float
    # Add all actual feature names used during training

# Step 5: Define endpoint
@app.post("/score")
def score_transaction(tx: Transaction):
    tx_df = pd.DataFrame([tx.dict()])
    fraud_prob = model.predict_proba(tx_df)[0][1]
    shap_values = explainer(tx_df)

    # Extract top 3 contributing features
    top_feats = sorted(
        zip(tx_df.columns, shap_values.values[0]),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:3]

    return {
        "fraud_risk": fraud_prob,
        "top_features": [
            {"feature": feat, "impact": float(impact)} for feat, impact in top_feats
        ]
    }

code = """
from fastapi import FastAPI
from pydantic import BaseModel
import shap
import pandas as pd
from catboost import CatBoostClassifier

model = CatBoostClassifier()
model.load_model("catboost_fraud_model.cbm")  # make sure this model file exists

explainer = shap.Explainer(model)

app = FastAPI()

class Transaction(BaseModel):
    feature1: float
    feature2: float
    feature3: float
    # Add all other features used during training

@app.post("/score")
def score_transaction(tx: Transaction):
    tx_df = pd.DataFrame([tx.dict()])
    fraud_prob = model.predict_proba(tx_df)[0][1]
    shap_values = explainer(tx_df)
    top_feats = sorted(
        zip(tx_df.columns, shap_values.values[0]),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:3]
    return {
        "fraud_risk": fraud_prob,
        "top_features": [{"feature": f[0], "impact": f[1]} for f in top_feats]
    }
"""

# Save it to fraud_api.py
with open("fraud_api.py", "w") as f:
    f.write(code)

!uvicorn fraud_api:app --reload

!pip install pyngrok

print(model.feature_names_)

class Transaction(BaseModel):
    Time: float
    Amount: float
    TransactionType: float
    # all other features exactly matching model.feature_names_

@app.post("/score")
def score_transaction(tx: Transaction):
    tx_df = pd.DataFrame([tx.model_dump()])  # Use model_dump() for Pydantic v2+
    # Make sure tx_df columns order matches model.feature_names_
    tx_df = tx_df[model.feature_names_]

    fraud_prob = model.predict_proba(tx_df)[0][1]
    shap_values = explainer(tx_df)

    top_feats = sorted(
        zip(tx_df.columns, shap_values.values[0]),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:3]

    response = {
        "fraud_risk": fraud_prob,
        "top_features": [{"feature": f[0], "impact": f[1]} for f in top_feats]
    }
    return response

!pip install uvicorn

# Commented out IPython magic to ensure Python compatibility.
# %%writefile fraud_api.py
# from fastapi import FastAPI
# from pydantic import BaseModel
# import shap
# import pandas as pd
# from catboost import CatBoostClassifier
# 
# model = CatBoostClassifier()
# model.load_model("catboost_fraud_model.cbm")
# 
# explainer = shap.Explainer(model)
# 
# app = FastAPI()
# 
# class Transaction(BaseModel):
#     Time: float
#     V1: float
#     V2: float
#     V3: float
#     V4: float
#     V5: float
#     V6: float
#     V7: float
#     V8: float
#     V9: float
#     V10: float
#     V11: float
#     V12: float
#     V13: float
#     V14: float
#     V15: float
#     V16: float
#     V17: float
#     V18: float
#     V19: float
#     V20: float
#     V21: float
#     V22: float
#     V23: float
#     V24: float
#     V25: float
#     V26: float
#     V27: float
#     V28: float
#     Amount: float
# 
# @app.post("/score")
# def score_transaction(tx: Transaction):
#     tx_df = pd.DataFrame([tx.model_dump()])
#     tx_df = tx_df[model.feature_names_]
# 
#     fraud_prob = model.predict_proba(tx_df)[0][1]
#     shap_values = explainer(tx_df)
# 
#     top_feats = sorted(
#         zip(tx_df.columns, shap_values.values[0]),
#         key=lambda x: abs(x[1]),
#         reverse=True
#     )[:3]
# 
#     return {
#         "fraud_risk": fraud_prob,
#         "top_features": [{"feature": f[0], "impact": f[1]} for f in top_feats]
#     }
#

!uvicorn fraud_api:app --host 0.0.0.0 --port 8000 --reload

!pip install fastapi uvicorn[standard] catboost shap

!uvicorn fraud_api:app --host 0.0.0.0 --port 8000 --reload

@app.post("/submit_feedback/")
def submit_feedback(transaction_id: str, helpful: bool, notes: Optional[str] = None):
    # Store feedback locally or in a DB
    feedback_store.append({
        "transaction_id": transaction_id,
        "helpful": helpful,
        "notes": notes,
        "timestamp": datetime.utcnow().isoformat()
    })
    return {"message": "Feedback submitted successfully"}

def generate_prompt(transaction, prior_feedback=None):
    prompt = f"""You are a fraud analyst assistant.
Transaction: {transaction}

"""
    if prior_feedback:
        prompt += f"Analyst feedback: {prior_feedback}\n"
        prompt += "Improve clarity, focus on similar past fraud reasons.\n"

    return prompt

{
  "instruction": "Explain why this transaction is fraudulent",
  "input": "Transaction: ID123 - $999 to XYZ, 2 AM, overseas IP, prior fraud tag.",
  "output": "This transaction is suspicious due to late-night overseas access and a known fraud tag."
}

from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments

# Load base model
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")

# Apply LoRA
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, peft_config)

# Train
trainer = Trainer(
    model=model,
    args=TrainingArguments(...),
    train_dataset=train_dataset
)
trainer.train()

#MLOPS

!pip install catboost
from catboost import CatBoostClassifier
import pandas as pd
import joblib

# Load and preprocess data
data = pd.read_csv('/content/creditcard.csv')
X = data.drop('Class', axis=1)
y = data['Class']

# Train CatBoost
model = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, verbose=0)
model.fit(X, y)
import os
if not os.path.exists('models'):
    os.makedirs('models')
# Save model
model.save_model('models/fraud_model.cbm')

!pip install faiss-cpu sentence-transformers

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')
docs = ["Suspicious login", "Repeated small transactions", ...]  # Sample fraud-related cases
embeddings = model.encode(docs)

# Create FAISS index
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)
faiss.write_index(index, "models/faiss_index.faiss")

import shap
from catboost import CatBoostClassifier, Pool
import pandas as pd

model = CatBoostClassifier()
model.load_model("models/fraud_model.cbm")

X = pd.read_csv("/content/creditcard.csv").drop("Class", axis=1)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# Example: Get SHAP explanation for first row
print(shap_values[0])

!pip install fastapi uvicorn pydantic shap catboost sentence-transformers faiss-cpu

from fastapi import FastAPI
from pydantic import BaseModel
import faiss, shap, json
from catboost import CatBoostClassifier
from sentence_transformers import SentenceTransformer

app = FastAPI()

# Load models
model = CatBoostClassifier()
model.load_model("models/fraud_model.cbm")
embedder = SentenceTransformer('all-MiniLM-L6-v2')
faiss_index = faiss.read_index("models/faiss_index.faiss")

class Transaction(BaseModel):
    features: dict
    query: str

@app.post("/predict/")
def predict(data: Transaction):
    # Predict fraud
    X = pd.DataFrame([data.features])
    pred = model.predict(X)[0]

    # SHAP explanation
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)

    # Retrieve similar case
    embedding = embedder.encode([data.query])
    _, idx = faiss_index.search(np.array(embedding), k=1)

    return {
        "prediction": int(pred),
        "top_similar_case_index": int(idx[0][0]),
        "shap_values": shap_values[0].tolist()
    }

#code for the project#

# Step 1: Data Preprocessing & Fraud Detection Model (XGBoost)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import xgboost as xgb
import joblib
import shap
import mlflow
import mlflow.sklearn

# Load the dataset (replace with actual path or input source)
data = pd.read_csv("")  # Placeholder CSV file

# Basic preprocessing (customize as needed)
data = data.dropna()
X = data.drop("is_fraud", axis=1)
y = data["is_fraud"]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Start MLflow experiment tracking
mlflow.set_experiment("fraud_detection_xgboost")
with mlflow.start_run():
    model = xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)
    model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print("Accuracy:", acc)
    print(classification_report(y_test, y_pred))

    # Log model and metrics
    mlflow.log_metric("accuracy", acc)
    mlflow.sklearn.log_model(model, "model")

    # Save model locally
    joblib.dump(model, "xgb_fraud_model.pkl")

    # SHAP Explanation
    explainer = shap.Explainer(model)
    shap_values = explainer(X_test)
    shap.summary_plot(shap_values, X_test)  # Optional interactive plot

from google.colab import drive
drive.mount('/content/drive')

# Step 2: Document Embedding & FAISS Vector Store for RAG

from sentence_transformers import SentenceTransformer
from sklearn.datasets import load_files
import faiss
import numpy as np
import pickle
import os

# Load your corpus (example: legal documents, transaction policies, fraud SOPs)
# Assume we have .txt files in a 'docs/' folder
doc_folder = "docs/"
doc_texts = []
doc_names = []

for filename in os.listdir(doc_folder):
    with open(os.path.join(doc_folder, filename), 'r', encoding='utf-8') as file:
        content = file.read()
        doc_texts.append(content)
        doc_names.append(filename)

# Embed documents using SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")
doc_embeddings = model.encode(doc_texts, convert_to_tensor=False)

# Convert to NumPy array for FAISS
doc_embeddings = np.array(doc_embeddings).astype('float32')

# Initialize FAISS index
embedding_dim = doc_embeddings.shape[1]
index = faiss.IndexFlatL2(embedding_dim)
index.add(doc_embeddings)

# Save FAISS index and metadata
faiss.write_index(index, "faiss_index.index")
with open("doc_metadata.pkl", "wb") as f:
    pickle.dump(doc_names, f)

print("FAISS index and document metadata saved.")

# Function to query FAISS
def retrieve_top_k(query, k=3):
    query_embedding = model.encode([query], convert_to_tensor=False).astype('float32')
    D, I = index.search(query_embedding, k)
    top_docs = [doc_texts[i] for i in I[0]]
    top_names = [doc_names[i] for i in I[0]]
    return list(zip(top_names, top_docs))

from fastapi import FastAPI, Request
from pydantic import BaseModel
import joblib
import shap
import numpy as np
import faiss
import pickle
from sentence_transformers import SentenceTransformer
import uvicorn

# Initialize app
app = FastAPI()

# Load fraud model
model = joblib.load("xgb_fraud_model.pkl")
explainer = shap.Explainer(model)

# Load FAISS index and doc metadata
faiss_index = faiss.read_index("faiss_index.index")
with open("doc_metadata.pkl", "rb") as f:
    doc_names = pickle.load(f)

# Load SentenceTransformer
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Sample docs (ensure same order as embedding)
with open("docs/sample_docs.pkl", "rb") as f:
    doc_texts = pickle.load(f)

# Input format
class FraudInput(BaseModel):
    features: list
    query: str

@app.post("/fraud-analyze")
def analyze(input_data: FraudInput):
    # Fraud prediction
    x_input = np.array([input_data.features])
    pred = model.predict(x_input)[0]
    prob = model.predict_proba(x_input)[0][1]

    # SHAP explanation
    shap_vals = explainer(x_input)
    shap_scores = dict(zip(model.get_booster().feature_names, shap_vals.values[0]))

    # RAG Query
    query_embedding = embedder.encode([input_data.query], convert_to_tensor=False).astype("float32")
    D, I = faiss_index.search(query_embedding, k=3)
    retrieved_docs = [doc_texts[i] for i in I[0]]
    doc_results = [{"doc_name": doc_names[i], "content": retrieved_docs[idx]} for idx, i in enumerate(I[0])]

    # GPT-4 Placeholder Response
    gpt_answer = f"GPT-4 response based on retrieved documents: {input_data.query}..."

    return {
        "fraud_prediction": int(pred),
        "fraud_probability": float(prob),
        "shap_scores": shap_scores,
        "retrieved_documents": doc_results,
        "answer": gpt_answer
    }

# Run the app
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

# app.py

from fastapi import FastAPI, Request
from pydantic import BaseModel
import joblib
import shap
import pandas as pd
import numpy as np
import faiss
import pickle
from sentence_transformers import SentenceTransformer
import openai

# Load Fraud Detection Model
fraud_model = joblib.load("xgb_fraud_model.pkl")
explainer = shap.Explainer(fraud_model)

# Load FAISS index and document metadata
faiss_index = faiss.read_index("faiss_index.index")
with open("doc_metadata.pkl", "rb") as f:
    doc_names = pickle.load(f)

# Load SentenceTransformer
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Load Document Texts (you could persist and load them too)
import os
doc_texts = []
doc_folder = "docs/"
for filename in os.listdir(doc_folder):
    with open(os.path.join(doc_folder, filename), 'r', encoding='utf-8') as file:
        content = file.read()
        doc_texts.append(content)

# FastAPI app init
app = FastAPI()

# Request schema for fraud prediction
class FraudRequest(BaseModel):
    features: dict

# Request schema for document-based Q&A
class QueryRequest(BaseModel):
    question: str

# Fraud Prediction Endpoint
@app.post("/predict_fraud")
def predict_fraud(req: FraudRequest):
    df = pd.DataFrame([req.features])
    pred = fraud_model.predict(df)[0]
    prob = fraud_model.predict_proba(df)[0][1]

    shap_vals = explainer(df)
    shap_df = pd.DataFrame({
        "feature": df.columns,
        "value": df.iloc[0].values,
        "shap": shap_vals.values[0]
    }).sort_values("shap", key=abs, ascending=False)

    return {
        "prediction": int(pred),
        "fraud_probability": round(prob, 4),
        "explanation": shap_df.to_dict(orient="records")
    }

# FAISS-based Semantic Search
def retrieve_top_k_docs(query, k=3):
    query_embedding = embedding_model.encode([query], convert_to_tensor=False).astype("float32")
    D, I = faiss_index.search(query_embedding, k)
    top_docs = [doc_texts[i] for i in I[0]]
    return top_docs

# OpenAI GPT API Setup (replace with local LLM if needed)
openai.api_key = "YOUR_API_KEY"

def generate_answer_from_docs(question, docs):
    context = "\n\n".join(docs)
    prompt = f"Answer the question based on the context below:\n\n{context}\n\nQuestion: {question}\nAnswer:"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=300
    )
    return response.choices[0].message['content'].strip()

# RAG-based QA Endpoint
@app.post("/ask")
def ask_question(req: QueryRequest):
    top_docs = retrieve_top_k_docs(req.question)
    answer = generate_answer_from_docs(req.question, top_docs)
    return {"answer": answer, "source_docs": top_docs}

!pip install mlflow
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import xgboost as xgb
import joblib
import shap
import mlflow
import mlflow.sklearn

# Load dataset
data = pd.read_csv("/content/BankFAQs.csv")

# Basic preprocessing
data = data.dropna()
X = data.drop("Class", axis=1)
y = data["Class"]
from sklearn.preprocessing import LabelEncoder

data = data.dropna()
X = data.drop("Class", axis=1)
y = data["Class"]

le = LabelEncoder()
y_encoded = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

model = xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)


# MLflow experiment tracking
mlflow.set_experiment("fraud_detection_xgboost")
with mlflow.start_run():
    model = xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)
    model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print("Accuracy:", acc)
    print(classification_report(y_test, y_pred))

    # Log metrics and model
    mlflow.log_metric("accuracy", acc)
    mlflow.sklearn.log_model(model, "model")

    # Save model locally for later inference
    joblib.dump(model, "xgb_fraud_model.pkl")

    # SHAP explainer (optional)
    explainer = shap.Explainer(model)
    shap_values = explainer(X_test)
    shap.summary_plot(shap_values, X_test)

from sklearn.preprocessing import LabelEncoder

data = data.dropna()
X = data.drop("Class", axis=1)
y = data["Class"]

le = LabelEncoder()
y_encoded = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

model = xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import classification_report

# Load your dataset
data = pd.read_csv("/content/BankFAQs.csv")  # Replace with your actual file path

# Combine Question and Answer into one text feature
data['text'] = data['Question'] + " " + data['Answer']

# Encode class labels to integers
le = LabelEncoder()
data['label'] = le.fit_transform(data['Class'])

# Vectorize text data using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X = vectorizer.fit_transform(data['text'])
y = data['label']

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize and train XGBoost classifier
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)

model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

# Print classification report with original class names
print(classification_report(y_test, y_pred, target_names=le.classes_))

import joblib

# Save the trained model
joblib.dump(model, "xgb_text_classifier.pkl")

# Save the TF-IDF vectorizer
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

# Save the label encoder to decode predictions later
joblib.dump(le, "label_encoder.pkl")

import os

# Create the 'docs' folder if it doesn't exist
os.makedirs("docs", exist_ok=True)

sample_doc_1 = """IVR Fraud SOP:
1. Do not share OTP.
2. Mobile number must be registered.
3. Limit IVR attempts to 3.
"""

sample_doc_2 = """Transaction Guidelines:
- Check expiry and CVV.
- Add-on cards need primary holder's approval.
"""

with open("docs/sop1.txt", "w") as f:
    f.write(sample_doc_1)

with open("docs/sop2.txt", "w") as f:
    f.write(sample_doc_2)

!pip install faiss-cpu sentence-transformers

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import os
import pickle

# Load documents from folder
doc_folder = "docs/"
doc_texts = []
doc_names = []

for filename in os.listdir(doc_folder):
    with open(os.path.join(doc_folder, filename), 'r', encoding='utf-8') as f:
        doc_texts.append(f.read())
        doc_names.append(filename)

# Load embedding model
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Create embeddings
doc_embeddings = embedder.encode(doc_texts, convert_to_tensor=False)
doc_embeddings = np.array(doc_embeddings).astype('float32')

# Build FAISS index
embedding_dim = doc_embeddings.shape[1]
index = faiss.IndexFlatL2(embedding_dim)
index.add(doc_embeddings)

# Save FAISS index and metadata
faiss.write_index(index, "faiss_index.index")
with open("doc_metadata.pkl", "wb") as f:
    pickle.dump(doc_names, f)

print("FAISS index and document metadata saved.")

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import os
import pickle
import textwrap

def chunk_text(text, chunk_size=40):
    return textwrap.wrap(text, width=chunk_size, break_long_words=False)

# Load and chunk documents
doc_folder = "docs"
doc_chunks = []
chunk_sources = []

for filename in os.listdir(doc_folder):
    with open(os.path.join(doc_folder, filename), "r", encoding="utf-8") as f:
        content = f.read()
        chunks = chunk_text(content, chunk_size=300)  # chunk size in characters
        doc_chunks.extend(chunks)
        chunk_sources.extend([filename] * len(chunks))

# rag_qa.py

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle

# Load saved FAISS index and metadata
index = faiss.read_index("faiss_index.index")
with open("doc_metadata.pkl", "rb") as f:
    doc_names = pickle.load(f)

# Load original documents
doc_folder = "docs"
doc_texts = []
for name in doc_names:
    with open(f"{doc_folder}/{name}", "r", encoding="utf-8") as f:
        doc_texts.append(f.read())

# Load sentence transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

# RAG function
def answer_question(question, top_k=1):
    query_embedding = model.encode([question], convert_to_tensor=False).astype("float32")
    D, I = index.search(query_embedding, top_k)

    print("\nðŸ” Top Result(s):\n")
    for i in range(top_k):
        print(f"[{i+1}] From: {doc_names[I[0][i]]}")
        print(doc_texts[I[0][i]])
        print("=" * 60)

# Demo
while True:
    q = input("\nâ“ Ask your question (or type 'exit'): ")
    if q.lower() == "exit":
        break
    answer_question(q, top_k=1)

# Fraud Analyst Copilot: Unified System

import numpy as np
import joblib
import shap
import faiss
import mlflow
from fastapi import FastAPI
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score
import openai

# ------------------------ Load Assets ------------------------
fraud_model = joblib.load("fraud_xgb_model.pkl")
explainer = shap.TreeExplainer(fraud_model)
embed_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
documents = [...]  # List of internal SOPs, policies, etc.
document_embeddings = embed_model.encode(documents, convert_to_tensor=True)
faiss_index = faiss.IndexFlatL2(document_embeddings.shape[1])
faiss_index.add(document_embeddings.cpu().numpy())

# ------------------------ FastAPI Setup ------------------------
app = FastAPI()

class TransactionQuery(BaseModel):
    features: list
    query: str

feature_names = ["amount", "time", "merchant_id", "customer_id", "country_mismatch", "merchant_reputation"]  # Update as needed

def get_gpt_response(query, context):
    gpt_prompt = f"Answer the following question based on context:\n{context}\n\nQuestion: {query}"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": gpt_prompt}],
    )
    return response["choices"][0]["message"]["content"].strip()

@app.post("/analyze_transaction")
def analyze_transaction(data: TransactionQuery):
    features = np.array(data.features).reshape(1, -1)
    query = data.query

    # Step 1: Fraud Prediction
    prediction_proba = fraud_model.predict_proba(features)[0][1]
    prediction = int(prediction_proba > 0.5)

    # Step 2: SHAP Explainability
    shap_values = explainer.shap_values(features)
    shap_dict = dict(zip(feature_names, shap_values[0]))

    # Step 3: Semantic Retrieval
    query_embedding = embed_model.encode(query, convert_to_tensor=True)
    D, I = faiss_index.search(query_embedding.cpu().numpy(), top_k=3)
    retrieved_docs = [documents[i] for i in I[0]]

    # Step 4: GPT Answer Generation
    context = "\n".join(retrieved_docs)
    gpt_response = get_gpt_response(query, context)

    return {
        "fraud_prediction": prediction,
        "probability": prediction_proba,
        "explanations": shap_dict,
        "retrieved_documents": retrieved_docs,
        "answer": gpt_response
    }

# ------------------------ Training & Logging (Offline Phase) ------------------------
def train_and_log_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)

    with mlflow.start_run():
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("precision", precision)
        mlflow.log_metric("recall", recall)
        mlflow.sklearn.log_model(model, "xgboost_model")

    joblib.dump(model, "fraud_xgb_model.pkl")

# Mini GPT-style Language Model from Scratch (Colab-ready)

import torch
import torch.nn as nn
import torch.nn.functional as F

# Step 1: Load or create a small text corpus
text = """
Once upon a time, there was a brilliant fraud analyst who built a copilot using AI.
This copilot could detect anomalies, explain predictions using SHAP, and retrieve relevant past cases.
"""

# Step 2: Tokenization and vocabulary
chars = sorted(list(set(text)))
vocab_size = len(chars)
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for ch, i in stoi.items()}
encode = lambda s: [stoi[c] for c in s]
decode = lambda l: ''.join([itos[i] for i in l])

data = torch.tensor(encode(text), dtype=torch.long)

# Step 3: Create batches
block_size = 64
batch_size = 16

def get_batch():
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x, y

# Step 4: Define GPT-like model
class GPTLanguageModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, num_heads=2, num_layers=2):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.position_embedding = nn.Embedding(1024, embed_dim)
        self.transformer_blocks = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads),
            num_layers=num_layers
        )
        self.lm_head = nn.Linear(embed_dim, vocab_size)

    def forward(self, x):
        B, T = x.size()
        token_emb = self.token_embedding(x)
        pos = torch.arange(T, device=x.device).unsqueeze(0)
        pos_emb = self.position_embedding(pos)
        x = token_emb + pos_emb
        x = x.permute(1, 0, 2)
        x = self.transformer_blocks(x)
        x = x.permute(1, 0, 2)
        logits = self.lm_head(x)
        return logits

model = GPTLanguageModel(vocab_size)
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
loss_fn = nn.CrossEntropyLoss()

# Step 5: Training loop
for step in range(2000):
    xb, yb = get_batch()
    logits = model(xb)
    loss = loss_fn(logits.view(-1, vocab_size), yb.view(-1))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % 200 == 0:
        print(f"Step {step}, Loss: {loss.item():.4f}")

# Step 6: Text generation
def generate(model, idx, max_new_tokens):
    model.eval()
    for _ in range(max_new_tokens):
        logits = model(idx[:, -block_size:])
        logits = logits[:, -1, :]
        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        idx = torch.cat((idx, next_token), dim=1)
    return idx

context = torch.tensor([[stoi['O']]], dtype=torch.long)
output = generate(model, context, max_new_tokens=100)
print("\nGenerated text:\n", decode(output[0].tolist()))